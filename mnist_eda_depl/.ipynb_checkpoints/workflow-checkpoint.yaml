apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: mnist-random-forest-pipeline-2-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-11-24T06:53:55.558329',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Training pipeline for
      time series forecasting on household power consumption dataset.", "inputs":
      [{"name": "bucket"}, {"name": "path"}], "name": "MNIST Random Forest Pipeline
      - 2"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.0}
spec:
  entrypoint: mnist-random-forest-pipeline-2
  templates:
  - name: download-data
    container:
      args: [--bucket, '{{inputs.parameters.bucket}}', --training-data-path, '{{inputs.parameters.path}}',
        --output-data, /tmp/outputs/output_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'boto3' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def download_data(bucket, training_data_path, output_data_path):
            import boto3
            s3 = boto3.client("s3", region_name='ap-south-1')
            s3.download_file(bucket, training_data_path, output_data_path)

        import argparse
        _parser = argparse.ArgumentParser(prog='Download data', description='')
        _parser.add_argument("--bucket", dest="bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--training-data-path", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-data", dest="output_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = download_data(**_parsed_args)
      image: python:3.7-slim
    inputs:
      parameters:
      - {name: bucket}
      - {name: path}
    outputs:
      artifacts:
      - {name: download-data-output_data, path: /tmp/outputs/output_data/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Download Raw Data, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--bucket", {"inputValue": "bucket"}, "--training-data-path",
          {"inputValue": "training_data_path"}, "--output-data", {"outputPath": "output_data"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''boto3'' --user)
          && \"$0\" \"$@\"", "python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef download_data(bucket, training_data_path, output_data_path):\n    import
          boto3\n    s3 = boto3.client(\"s3\", region_name=''ap-south-1'')\n    s3.download_file(bucket,
          training_data_path, output_data_path)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Download
          data'', description='''')\n_parser.add_argument(\"--bucket\", dest=\"bucket\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-data-path\",
          dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-data\",
          dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = download_data(**_parsed_args)\n"], "image": "python:3.7-slim"}}, "inputs":
          [{"name": "bucket", "type": "String"}, {"name": "training_data_path", "type":
          "String"}], "name": "Download data", "outputs": [{"name": "output_data",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: mnist-random-forest-pipeline-2
    inputs:
      parameters:
      - {name: bucket}
      - {name: path}
    dag:
      tasks:
      - name: download-data
        template: download-data
        arguments:
          parameters:
          - {name: bucket, value: '{{inputs.parameters.bucket}}'}
          - {name: path, value: '{{inputs.parameters.path}}'}
      - name: normalize-data
        template: normalize-data
        dependencies: [split-data]
        arguments:
          artifacts:
          - {name: split-data-output_test_data, from: '{{tasks.split-data.outputs.artifacts.split-data-output_test_data}}'}
          - {name: split-data-output_training_data, from: '{{tasks.split-data.outputs.artifacts.split-data-output_training_data}}'}
      - name: split-data
        template: split-data
        dependencies: [download-data]
        arguments:
          artifacts:
          - {name: download-data-output_data, from: '{{tasks.download-data.outputs.artifacts.download-data-output_data}}'}
      - name: train
        template: train
        dependencies: [normalize-data, split-data]
        arguments:
          artifacts:
          - {name: normalize-data-normalized_test_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_test_data}}'}
          - {name: normalize-data-normalized_training_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_training_data}}'}
          - {name: split-data-output_test_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_test_labels}}'}
          - {name: split-data-output_training_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_training_labels}}'}
      - name: train-2
        template: train-2
        dependencies: [normalize-data, split-data]
        arguments:
          artifacts:
          - {name: normalize-data-normalized_test_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_test_data}}'}
          - {name: normalize-data-normalized_training_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_training_data}}'}
          - {name: split-data-output_test_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_test_labels}}'}
          - {name: split-data-output_training_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_training_labels}}'}
      - name: train-3
        template: train-3
        dependencies: [normalize-data, split-data]
        arguments:
          artifacts:
          - {name: normalize-data-normalized_test_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_test_data}}'}
          - {name: normalize-data-normalized_training_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_training_data}}'}
          - {name: split-data-output_test_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_test_labels}}'}
          - {name: split-data-output_training_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_training_labels}}'}
      - name: train-4
        template: train-4
        dependencies: [normalize-data, split-data]
        arguments:
          artifacts:
          - {name: normalize-data-normalized_test_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_test_data}}'}
          - {name: normalize-data-normalized_training_data, from: '{{tasks.normalize-data.outputs.artifacts.normalize-data-normalized_training_data}}'}
          - {name: split-data-output_test_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_test_labels}}'}
          - {name: split-data-output_training_labels, from: '{{tasks.split-data.outputs.artifacts.split-data-output_training_labels}}'}
  - name: normalize-data
    container:
      args: [--training-data, /tmp/inputs/training_data/data, --test-data, /tmp/inputs/test_data/data,
        --normalized-training-data, /tmp/outputs/normalized_training_data/data, --normalized-test-data,
        /tmp/outputs/normalized_test_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef normalize_data(\n    training_data_path, \n    test_data_path, \n  \
        \  normalized_training_data, \n    normalized_test_data\n):\n    import pandas\
        \ as pd\n\n    x_train = pd.read_csv(training_data_path)\n    x_test = pd.read_csv(test_data_path)\n\
        \n    x_train = x_train/255.0\n    x_test = x_test/255.0\n\n    x_train.to_csv(normalized_training_data,\
        \ index=False)\n    x_test.to_csv(normalized_test_data, index=False)\n\n \
        \   print(\"ALL DONE\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Normalize\
        \ data', description='')\n_parser.add_argument(\"--training-data\", dest=\"\
        training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-data\", dest=\"test_data_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--normalized-training-data\"\
        , dest=\"normalized_training_data\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--normalized-test-data\"\
        , dest=\"normalized_test_data\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = normalize_data(**_parsed_args)\n"
      image: python:3.7-slim
    inputs:
      artifacts:
      - {name: split-data-output_test_data, path: /tmp/inputs/test_data/data}
      - {name: split-data-output_training_data, path: /tmp/inputs/training_data/data}
    outputs:
      artifacts:
      - {name: normalize-data-normalized_test_data, path: /tmp/outputs/normalized_test_data/data}
      - {name: normalize-data-normalized_training_data, path: /tmp/outputs/normalized_training_data/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Normalize Data, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--training-data", {"inputPath": "training_data"},
          "--test-data", {"inputPath": "test_data"}, "--normalized-training-data",
          {"outputPath": "normalized_training_data"}, "--normalized-test-data", {"outputPath":
          "normalized_test_data"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef normalize_data(\n    training_data_path, \n    test_data_path,
          \n    normalized_training_data, \n    normalized_test_data\n):\n    import
          pandas as pd\n\n    x_train = pd.read_csv(training_data_path)\n    x_test
          = pd.read_csv(test_data_path)\n\n    x_train = x_train/255.0\n    x_test
          = x_test/255.0\n\n    x_train.to_csv(normalized_training_data, index=False)\n    x_test.to_csv(normalized_test_data,
          index=False)\n\n    print(\"ALL DONE\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Normalize
          data'', description='''')\n_parser.add_argument(\"--training-data\", dest=\"training_data_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-data\",
          dest=\"test_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--normalized-training-data\",
          dest=\"normalized_training_data\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--normalized-test-data\",
          dest=\"normalized_test_data\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = normalize_data(**_parsed_args)\n"], "image": "python:3.7-slim"}}, "inputs":
          [{"name": "training_data", "type": "CSV"}, {"name": "test_data", "type":
          "CSV"}], "name": "Normalize data", "outputs": [{"name": "normalized_training_data",
          "type": "CSV"}, {"name": "normalized_test_data", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: split-data
    container:
      args: [--input-data, /tmp/inputs/input_data/data, --output-training-data, /tmp/outputs/output_training_data/data,
        --output-training-labels, /tmp/outputs/output_training_labels/data, --output-test-data,
        /tmp/outputs/output_test_data/data, --output-test-labels, /tmp/outputs/output_test_labels/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef split_data(\n    input_data_path, \n    output_training_data, \n   \
        \ output_training_labels, \n    output_test_data, \n    output_test_labels\n\
        ):\n    import pandas as pd\n    df_train = df = pd.read_csv(input_data_path,\
        \ sep=',', header=0)\n\n    x = df_train.iloc[:, 1:]\n    y = df_train['label'].tolist()\n\
        \n    # # Select 10000 rows data as a testing dataset\n    x_test = x.iloc[0:10000,\
        \ :].values.astype('float32') # all pixel values \n    y_test = y[0:10000]\
        \ # Select label for testing data\n    x_train = x.iloc[10000:, :].values.astype('float32')\
        \ # all pixel values \n    y_train = y[10000:]\n\n    pd.DataFrame(x_train).to_csv(output_training_data,\
        \ index=False)\n    pd.DataFrame(y_train).to_csv(output_training_labels, index=False)\n\
        \n    pd.DataFrame(x_test).to_csv(output_test_data, index=False)\n    pd.DataFrame(y_test).to_csv(output_test_labels,\
        \ index=False)\n    print(\"ALL DONE\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Split\
        \ data', description='')\n_parser.add_argument(\"--input-data\", dest=\"input_data_path\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --output-training-data\", dest=\"output_training_data\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-training-labels\"\
        , dest=\"output_training_labels\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-test-data\"\
        , dest=\"output_test_data\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-test-labels\"\
        , dest=\"output_test_labels\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = split_data(**_parsed_args)\n"
      image: python:3.7-slim
    inputs:
      artifacts:
      - {name: download-data-output_data, path: /tmp/inputs/input_data/data}
    outputs:
      artifacts:
      - {name: split-data-output_test_data, path: /tmp/outputs/output_test_data/data}
      - {name: split-data-output_test_labels, path: /tmp/outputs/output_test_labels/data}
      - {name: split-data-output_training_data, path: /tmp/outputs/output_training_data/data}
      - {name: split-data-output_training_labels, path: /tmp/outputs/output_training_labels/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Split Data, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--input-data", {"inputPath": "input_data"}, "--output-training-data",
          {"outputPath": "output_training_data"}, "--output-training-labels", {"outputPath":
          "output_training_labels"}, "--output-test-data", {"outputPath": "output_test_data"},
          "--output-test-labels", {"outputPath": "output_test_labels"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "python3", "-u", "-c", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef split_data(\n    input_data_path, \n    output_training_data,
          \n    output_training_labels, \n    output_test_data, \n    output_test_labels\n):\n    import
          pandas as pd\n    df_train = df = pd.read_csv(input_data_path, sep='','',
          header=0)\n\n    x = df_train.iloc[:, 1:]\n    y = df_train[''label''].tolist()\n\n    #
          # Select 10000 rows data as a testing dataset\n    x_test = x.iloc[0:10000,
          :].values.astype(''float32'') # all pixel values \n    y_test = y[0:10000]
          # Select label for testing data\n    x_train = x.iloc[10000:, :].values.astype(''float32'')
          # all pixel values \n    y_train = y[10000:]\n\n    pd.DataFrame(x_train).to_csv(output_training_data,
          index=False)\n    pd.DataFrame(y_train).to_csv(output_training_labels, index=False)\n\n    pd.DataFrame(x_test).to_csv(output_test_data,
          index=False)\n    pd.DataFrame(y_test).to_csv(output_test_labels, index=False)\n    print(\"ALL
          DONE\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Split
          data'', description='''')\n_parser.add_argument(\"--input-data\", dest=\"input_data_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-training-data\",
          dest=\"output_training_data\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-training-labels\",
          dest=\"output_training_labels\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-test-data\",
          dest=\"output_test_data\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-test-labels\",
          dest=\"output_test_labels\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = split_data(**_parsed_args)\n"], "image": "python:3.7-slim"}}, "inputs":
          [{"name": "input_data", "type": "CSV"}], "name": "Split data", "outputs":
          [{"name": "output_training_data", "type": "CSV"}, {"name": "output_training_labels",
          "type": "CSV"}, {"name": "output_test_data", "type": "CSV"}, {"name": "output_test_labels",
          "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: train
    container:
      args: [--n-estimators, '100', --depth, '2', --random-state, '0', --mlflow-tracking-uri,
        'http://mlflow-service.kubeflow.svc.cluster.local:80', --mlflow-experiment-name,
        mnist_random_forest, --model-name, MNIST_RF_NEW, --training-data, /tmp/inputs/training_data/data,
        --training-labels, /tmp/inputs/training_labels/data, --test-data, /tmp/inputs/test_data/data,
        --test-labels, /tmp/inputs/test_labels/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'mlflow' 'pandas' 'sklearn' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'mlflow' 'pandas' 'sklearn'
        'boto3' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n\
        \    mlflow_experiment_name,\n    model_name,\n    training_data_path, \n\
        \    training_labels_path, \n    test_data_path,\n    test_labels_path\n \
        \   ):\n\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    x_train = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\
        \n    x_test = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\
        \n    print(x_train, y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,\
        \ max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest\
        \ algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred\
        \ = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n\
        \    wrong = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong\
        \ += 1 if pred != truth else 0\n\n    accuracy = ((total - wrong)/total) *\
        \ 100.0\n\n    ########## log model ####################\n    import mlflow\n\
        \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\
        \n    with mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\"\
        , n_estimators)\n        mlflow.log_param(\"max_depth\", depth)\n        mlflow.log_param(\"\
        random_state\", random_state)\n        mlflow.log_metric(\"accuracy\", accuracy)\n\
        \n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\
        \n        model_artifact_location = run.info.artifact_uri + \"/model\"\n \
        \       logged_model = 'runs:/'+run.info.run_id+'/model'\n\n    return { 'accuracy':\
        \ accuracy, 'model_path': logged_model }\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train',\
        \ description='')\n_parser.add_argument(\"--n-estimators\", dest=\"n_estimators\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --depth\", dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--random-state\", dest=\"random_state\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\"\
        , dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlflow-experiment-name\", dest=\"mlflow_experiment_name\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\"\
        , dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-data\", dest=\"test_data_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\", dest=\"\
        test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"
      image: python:3.7-slim
      resources:
        requests: {memory: 1G, cpu: '0.5'}
    inputs:
      artifacts:
      - {name: normalize-data-normalized_test_data, path: /tmp/inputs/test_data/data}
      - {name: split-data-output_test_labels, path: /tmp/inputs/test_labels/data}
      - {name: normalize-data-normalized_training_data, path: /tmp/inputs/training_data/data}
      - {name: split-data-output_training_labels, path: /tmp/inputs/training_labels/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: 'Model Training: 0',
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--n-estimators", {"inputValue": "n_estimators"}, "--depth", {"inputValue":
          "depth"}, "--random-state", {"inputValue": "random_state"}, "--mlflow-tracking-uri",
          {"inputValue": "mlflow_tracking_uri"}, "--mlflow-experiment-name", {"inputValue":
          "mlflow_experiment_name"}, "--model-name", {"inputValue": "model_name"},
          "--training-data", {"inputPath": "training_data"}, "--training-labels",
          {"inputPath": "training_labels"}, "--test-data", {"inputPath": "test_data"},
          "--test-labels", {"inputPath": "test_labels"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''mlflow'' ''pandas'' ''sklearn'' ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''mlflow'' ''pandas''
          ''sklearn'' ''boto3'' --user) && \"$0\" \"$@\"", "python3", "-u", "-c",
          "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n    mlflow_experiment_name,\n    model_name,\n    training_data_path,
          \n    training_labels_path, \n    test_data_path,\n    test_labels_path\n    ):\n\n    import
          pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\n    x_train
          = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\n    x_test
          = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\n    print(x_train,
          y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,
          max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest
          algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred
          = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n    wrong
          = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong += 1 if
          pred != truth else 0\n\n    accuracy = ((total - wrong)/total) * 100.0\n\n    ##########
          log model ####################\n    import mlflow\n\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\n    with
          mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        mlflow.log_param(\"max_depth\",
          depth)\n        mlflow.log_param(\"random_state\", random_state)\n        mlflow.log_metric(\"accuracy\",
          accuracy)\n\n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\n        model_artifact_location
          = run.info.artifact_uri + \"/model\"\n        logged_model = ''runs:/''+run.info.run_id+''/model''\n\n    return
          { ''accuracy'': accuracy, ''model_path'': logged_model }\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train'', description='''')\n_parser.add_argument(\"--n-estimators\",
          dest=\"n_estimators\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--depth\",
          dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-state\",
          dest=\"random_state\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\",
          dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-experiment-name\",
          dest=\"mlflow_experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-data\",
          dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\",
          dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-data\",
          dest=\"test_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\",
          dest=\"test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"], "image":
          "python:3.7-slim"}}, "inputs": [{"name": "n_estimators", "type": "Integer"},
          {"name": "depth", "type": "Integer"}, {"name": "random_state", "type": "Integer"},
          {"name": "mlflow_tracking_uri", "type": "String"}, {"name": "mlflow_experiment_name",
          "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "training_data",
          "type": "CSV"}, {"name": "training_labels", "type": "CSV"}, {"name": "test_data",
          "type": "CSV"}, {"name": "test_labels", "type": "CSV"}], "name": "Train"}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-2
    container:
      args: [--n-estimators, '500', --depth, '4', --random-state, '0', --mlflow-tracking-uri,
        'http://mlflow-service.kubeflow.svc.cluster.local:80', --mlflow-experiment-name,
        mnist_random_forest, --model-name, MNIST_RF_NEW, --training-data, /tmp/inputs/training_data/data,
        --training-labels, /tmp/inputs/training_labels/data, --test-data, /tmp/inputs/test_data/data,
        --test-labels, /tmp/inputs/test_labels/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'mlflow' 'pandas' 'sklearn' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'mlflow' 'pandas' 'sklearn'
        'boto3' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n\
        \    mlflow_experiment_name,\n    model_name,\n    training_data_path, \n\
        \    training_labels_path, \n    test_data_path,\n    test_labels_path\n \
        \   ):\n\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    x_train = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\
        \n    x_test = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\
        \n    print(x_train, y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,\
        \ max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest\
        \ algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred\
        \ = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n\
        \    wrong = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong\
        \ += 1 if pred != truth else 0\n\n    accuracy = ((total - wrong)/total) *\
        \ 100.0\n\n    ########## log model ####################\n    import mlflow\n\
        \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\
        \n    with mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\"\
        , n_estimators)\n        mlflow.log_param(\"max_depth\", depth)\n        mlflow.log_param(\"\
        random_state\", random_state)\n        mlflow.log_metric(\"accuracy\", accuracy)\n\
        \n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\
        \n        model_artifact_location = run.info.artifact_uri + \"/model\"\n \
        \       logged_model = 'runs:/'+run.info.run_id+'/model'\n\n    return { 'accuracy':\
        \ accuracy, 'model_path': logged_model }\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train',\
        \ description='')\n_parser.add_argument(\"--n-estimators\", dest=\"n_estimators\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --depth\", dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--random-state\", dest=\"random_state\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\"\
        , dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlflow-experiment-name\", dest=\"mlflow_experiment_name\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\"\
        , dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-data\", dest=\"test_data_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\", dest=\"\
        test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"
      image: python:3.7-slim
      resources:
        requests: {memory: 1G, cpu: '0.5'}
    inputs:
      artifacts:
      - {name: normalize-data-normalized_test_data, path: /tmp/inputs/test_data/data}
      - {name: split-data-output_test_labels, path: /tmp/inputs/test_labels/data}
      - {name: normalize-data-normalized_training_data, path: /tmp/inputs/training_data/data}
      - {name: split-data-output_training_labels, path: /tmp/inputs/training_labels/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: 'Model Training: 1',
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--n-estimators", {"inputValue": "n_estimators"}, "--depth", {"inputValue":
          "depth"}, "--random-state", {"inputValue": "random_state"}, "--mlflow-tracking-uri",
          {"inputValue": "mlflow_tracking_uri"}, "--mlflow-experiment-name", {"inputValue":
          "mlflow_experiment_name"}, "--model-name", {"inputValue": "model_name"},
          "--training-data", {"inputPath": "training_data"}, "--training-labels",
          {"inputPath": "training_labels"}, "--test-data", {"inputPath": "test_data"},
          "--test-labels", {"inputPath": "test_labels"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''mlflow'' ''pandas'' ''sklearn'' ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''mlflow'' ''pandas''
          ''sklearn'' ''boto3'' --user) && \"$0\" \"$@\"", "python3", "-u", "-c",
          "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n    mlflow_experiment_name,\n    model_name,\n    training_data_path,
          \n    training_labels_path, \n    test_data_path,\n    test_labels_path\n    ):\n\n    import
          pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\n    x_train
          = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\n    x_test
          = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\n    print(x_train,
          y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,
          max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest
          algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred
          = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n    wrong
          = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong += 1 if
          pred != truth else 0\n\n    accuracy = ((total - wrong)/total) * 100.0\n\n    ##########
          log model ####################\n    import mlflow\n\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\n    with
          mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        mlflow.log_param(\"max_depth\",
          depth)\n        mlflow.log_param(\"random_state\", random_state)\n        mlflow.log_metric(\"accuracy\",
          accuracy)\n\n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\n        model_artifact_location
          = run.info.artifact_uri + \"/model\"\n        logged_model = ''runs:/''+run.info.run_id+''/model''\n\n    return
          { ''accuracy'': accuracy, ''model_path'': logged_model }\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train'', description='''')\n_parser.add_argument(\"--n-estimators\",
          dest=\"n_estimators\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--depth\",
          dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-state\",
          dest=\"random_state\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\",
          dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-experiment-name\",
          dest=\"mlflow_experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-data\",
          dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\",
          dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-data\",
          dest=\"test_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\",
          dest=\"test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"], "image":
          "python:3.7-slim"}}, "inputs": [{"name": "n_estimators", "type": "Integer"},
          {"name": "depth", "type": "Integer"}, {"name": "random_state", "type": "Integer"},
          {"name": "mlflow_tracking_uri", "type": "String"}, {"name": "mlflow_experiment_name",
          "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "training_data",
          "type": "CSV"}, {"name": "training_labels", "type": "CSV"}, {"name": "test_data",
          "type": "CSV"}, {"name": "test_labels", "type": "CSV"}], "name": "Train"}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-3
    container:
      args: [--n-estimators, '800', --depth, '3', --random-state, '0', --mlflow-tracking-uri,
        'http://mlflow-service.kubeflow.svc.cluster.local:80', --mlflow-experiment-name,
        mnist_random_forest, --model-name, MNIST_RF_NEW, --training-data, /tmp/inputs/training_data/data,
        --training-labels, /tmp/inputs/training_labels/data, --test-data, /tmp/inputs/test_data/data,
        --test-labels, /tmp/inputs/test_labels/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'mlflow' 'pandas' 'sklearn' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'mlflow' 'pandas' 'sklearn'
        'boto3' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n\
        \    mlflow_experiment_name,\n    model_name,\n    training_data_path, \n\
        \    training_labels_path, \n    test_data_path,\n    test_labels_path\n \
        \   ):\n\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    x_train = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\
        \n    x_test = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\
        \n    print(x_train, y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,\
        \ max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest\
        \ algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred\
        \ = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n\
        \    wrong = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong\
        \ += 1 if pred != truth else 0\n\n    accuracy = ((total - wrong)/total) *\
        \ 100.0\n\n    ########## log model ####################\n    import mlflow\n\
        \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\
        \n    with mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\"\
        , n_estimators)\n        mlflow.log_param(\"max_depth\", depth)\n        mlflow.log_param(\"\
        random_state\", random_state)\n        mlflow.log_metric(\"accuracy\", accuracy)\n\
        \n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\
        \n        model_artifact_location = run.info.artifact_uri + \"/model\"\n \
        \       logged_model = 'runs:/'+run.info.run_id+'/model'\n\n    return { 'accuracy':\
        \ accuracy, 'model_path': logged_model }\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train',\
        \ description='')\n_parser.add_argument(\"--n-estimators\", dest=\"n_estimators\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --depth\", dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--random-state\", dest=\"random_state\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\"\
        , dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlflow-experiment-name\", dest=\"mlflow_experiment_name\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\"\
        , dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-data\", dest=\"test_data_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\", dest=\"\
        test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"
      image: python:3.7-slim
      resources:
        requests: {memory: 1G, cpu: '0.5'}
    inputs:
      artifacts:
      - {name: normalize-data-normalized_test_data, path: /tmp/inputs/test_data/data}
      - {name: split-data-output_test_labels, path: /tmp/inputs/test_labels/data}
      - {name: normalize-data-normalized_training_data, path: /tmp/inputs/training_data/data}
      - {name: split-data-output_training_labels, path: /tmp/inputs/training_labels/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: 'Model Training: 2',
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--n-estimators", {"inputValue": "n_estimators"}, "--depth", {"inputValue":
          "depth"}, "--random-state", {"inputValue": "random_state"}, "--mlflow-tracking-uri",
          {"inputValue": "mlflow_tracking_uri"}, "--mlflow-experiment-name", {"inputValue":
          "mlflow_experiment_name"}, "--model-name", {"inputValue": "model_name"},
          "--training-data", {"inputPath": "training_data"}, "--training-labels",
          {"inputPath": "training_labels"}, "--test-data", {"inputPath": "test_data"},
          "--test-labels", {"inputPath": "test_labels"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''mlflow'' ''pandas'' ''sklearn'' ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''mlflow'' ''pandas''
          ''sklearn'' ''boto3'' --user) && \"$0\" \"$@\"", "python3", "-u", "-c",
          "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n    mlflow_experiment_name,\n    model_name,\n    training_data_path,
          \n    training_labels_path, \n    test_data_path,\n    test_labels_path\n    ):\n\n    import
          pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\n    x_train
          = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\n    x_test
          = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\n    print(x_train,
          y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,
          max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest
          algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred
          = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n    wrong
          = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong += 1 if
          pred != truth else 0\n\n    accuracy = ((total - wrong)/total) * 100.0\n\n    ##########
          log model ####################\n    import mlflow\n\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\n    with
          mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        mlflow.log_param(\"max_depth\",
          depth)\n        mlflow.log_param(\"random_state\", random_state)\n        mlflow.log_metric(\"accuracy\",
          accuracy)\n\n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\n        model_artifact_location
          = run.info.artifact_uri + \"/model\"\n        logged_model = ''runs:/''+run.info.run_id+''/model''\n\n    return
          { ''accuracy'': accuracy, ''model_path'': logged_model }\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train'', description='''')\n_parser.add_argument(\"--n-estimators\",
          dest=\"n_estimators\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--depth\",
          dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-state\",
          dest=\"random_state\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\",
          dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-experiment-name\",
          dest=\"mlflow_experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-data\",
          dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\",
          dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-data\",
          dest=\"test_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\",
          dest=\"test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"], "image":
          "python:3.7-slim"}}, "inputs": [{"name": "n_estimators", "type": "Integer"},
          {"name": "depth", "type": "Integer"}, {"name": "random_state", "type": "Integer"},
          {"name": "mlflow_tracking_uri", "type": "String"}, {"name": "mlflow_experiment_name",
          "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "training_data",
          "type": "CSV"}, {"name": "training_labels", "type": "CSV"}, {"name": "test_data",
          "type": "CSV"}, {"name": "test_labels", "type": "CSV"}], "name": "Train"}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-4
    container:
      args: [--n-estimators, '50', --depth, '4', --random-state, '0', --mlflow-tracking-uri,
        'http://mlflow-service.kubeflow.svc.cluster.local:80', --mlflow-experiment-name,
        mnist_random_forest, --model-name, MNIST_RF_NEW, --training-data, /tmp/inputs/training_data/data,
        --training-labels, /tmp/inputs/training_labels/data, --test-data, /tmp/inputs/test_data/data,
        --test-labels, /tmp/inputs/test_labels/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'mlflow' 'pandas' 'sklearn' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'mlflow' 'pandas' 'sklearn'
        'boto3' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n\
        \    mlflow_experiment_name,\n    model_name,\n    training_data_path, \n\
        \    training_labels_path, \n    test_data_path,\n    test_labels_path\n \
        \   ):\n\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\
        \n    x_train = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\
        \n    x_test = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\
        \n    print(x_train, y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,\
        \ max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest\
        \ algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred\
        \ = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n\
        \    wrong = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong\
        \ += 1 if pred != truth else 0\n\n    accuracy = ((total - wrong)/total) *\
        \ 100.0\n\n    ########## log model ####################\n    import mlflow\n\
        \n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\
        \n    with mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\"\
        , n_estimators)\n        mlflow.log_param(\"max_depth\", depth)\n        mlflow.log_param(\"\
        random_state\", random_state)\n        mlflow.log_metric(\"accuracy\", accuracy)\n\
        \n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\
        \n        model_artifact_location = run.info.artifact_uri + \"/model\"\n \
        \       logged_model = 'runs:/'+run.info.run_id+'/model'\n\n    return { 'accuracy':\
        \ accuracy, 'model_path': logged_model }\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train',\
        \ description='')\n_parser.add_argument(\"--n-estimators\", dest=\"n_estimators\"\
        , type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --depth\", dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--random-state\", dest=\"random_state\", type=int,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\"\
        , dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlflow-experiment-name\", dest=\"mlflow_experiment_name\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\"\
        , dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-data\", dest=\"test_data_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\", dest=\"\
        test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"
      image: python:3.7-slim
      resources:
        requests: {memory: 1G, cpu: '0.5'}
    inputs:
      artifacts:
      - {name: normalize-data-normalized_test_data, path: /tmp/inputs/test_data/data}
      - {name: split-data-output_test_labels, path: /tmp/inputs/test_labels/data}
      - {name: normalize-data-normalized_training_data, path: /tmp/inputs/training_data/data}
      - {name: split-data-output_training_labels, path: /tmp/inputs/training_labels/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: 'Model Training: 3',
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--n-estimators", {"inputValue": "n_estimators"}, "--depth", {"inputValue":
          "depth"}, "--random-state", {"inputValue": "random_state"}, "--mlflow-tracking-uri",
          {"inputValue": "mlflow_tracking_uri"}, "--mlflow-experiment-name", {"inputValue":
          "mlflow_experiment_name"}, "--model-name", {"inputValue": "model_name"},
          "--training-data", {"inputPath": "training_data"}, "--training-labels",
          {"inputPath": "training_labels"}, "--test-data", {"inputPath": "test_data"},
          "--test-labels", {"inputPath": "test_labels"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''mlflow'' ''pandas'' ''sklearn'' ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''mlflow'' ''pandas''
          ''sklearn'' ''boto3'' --user) && \"$0\" \"$@\"", "python3", "-u", "-c",
          "def train(\n    n_estimators, \n    depth, \n    random_state, \n    mlflow_tracking_uri,\n    mlflow_experiment_name,\n    model_name,\n    training_data_path,
          \n    training_labels_path, \n    test_data_path,\n    test_labels_path\n    ):\n\n    import
          pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\n    x_train
          = pd.read_csv(training_data_path)\n    y_train = pd.read_csv(training_labels_path)\n\n    x_test
          = pd.read_csv(test_data_path)\n    y_test = pd.read_csv(test_labels_path)\n\n    print(x_train,
          y_train, x_test, y_test)\n    model_clf = RandomForestClassifier(n_estimators=n_estimators,
          max_depth=depth, random_state=random_state)\n\n    # Train the Random Forest
          algorithm\n    model_clf.fit(x_train, y_train)\n\n    # validate\n    y_pred
          = model_clf.predict(x_test)\n\n    # calculate accuracy\n    total = len(y_pred)\n    wrong
          = 0\n    for pred, truth in zip(y_pred, y_test):\n        wrong += 1 if
          pred != truth else 0\n\n    accuracy = ((total - wrong)/total) * 100.0\n\n    ##########
          log model ####################\n    import mlflow\n\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(mlflow_experiment_name)\n\n    with
          mlflow.start_run() as run:\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        mlflow.log_param(\"max_depth\",
          depth)\n        mlflow.log_param(\"random_state\", random_state)\n        mlflow.log_metric(\"accuracy\",
          accuracy)\n\n        mlflow.sklearn.log_model(model_clf, \"model\", registered_model_name=model_name)\n\n        model_artifact_location
          = run.info.artifact_uri + \"/model\"\n        logged_model = ''runs:/''+run.info.run_id+''/model''\n\n    return
          { ''accuracy'': accuracy, ''model_path'': logged_model }\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train'', description='''')\n_parser.add_argument(\"--n-estimators\",
          dest=\"n_estimators\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--depth\",
          dest=\"depth\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-state\",
          dest=\"random_state\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-tracking-uri\",
          dest=\"mlflow_tracking_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlflow-experiment-name\",
          dest=\"mlflow_experiment_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-data\",
          dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--training-labels\",
          dest=\"training_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-data\",
          dest=\"test_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-labels\",
          dest=\"test_labels_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"], "image":
          "python:3.7-slim"}}, "inputs": [{"name": "n_estimators", "type": "Integer"},
          {"name": "depth", "type": "Integer"}, {"name": "random_state", "type": "Integer"},
          {"name": "mlflow_tracking_uri", "type": "String"}, {"name": "mlflow_experiment_name",
          "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "training_data",
          "type": "CSV"}, {"name": "training_labels", "type": "CSV"}, {"name": "test_data",
          "type": "CSV"}, {"name": "test_labels", "type": "CSV"}], "name": "Train"}',
        pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: bucket}
    - {name: path}
  serviceAccountName: pipeline-runner
